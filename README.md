# Sales_Data_Pipeline

![Build Status](https://img.shields.io/badge/build-passing-brightgreen)
![License](https://img.shields.io/badge/license-MIT-blue)
![Airflow](https://img.shields.io/badge/orchestrator-Airflow-orange)
![dbt](https://img.shields.io/badge/transform-dbt-red)

## ğŸ“Œ Project Overview
This project implements an **end-to-end data pipeline** for sales analytics using **PostgreSQL, dbt, and Apache Airflow**.  
It ingests sales data from CSV files into PostgreSQL, transforms it with dbt into **staging, dimension, and fact tables**, and orchestrates the workflow using Airflow.  

The result is a structured analytics-ready dataset (customers, products, sales) that can be used for reporting and BI dashboards.  

---

## ğŸ—ï¸ Architecture
![Architecture Diagram](./docs/architecture.png)  
*(Placeholder â€” add your diagram here)*  

**Workflow**:
1. Load raw CSV â†’ PostgreSQL (`loaddata.py`).
2. Airflow DAG (`dbt_dag`) triggers dbt.
3. dbt creates staging, dimension, and fact models.
4. Data becomes analytics-ready in PostgreSQL schema.

---

## âš™ï¸ Tech Stack
- **PostgreSQL** â†’ Database for raw + transformed data  
- **dbt (Data Build Tool)** â†’ SQL-based transformations and testing  
- **Apache Airflow** â†’ Orchestration and scheduling (DAGs)  
- **Docker & Docker Compose** â†’ Containerized environment  
- **Python (psycopg2, pandas)** â†’ Load CSV into Postgres  

---

## ğŸ“‚ Data Sources
- **Batch CSV**: `train.csv` (Sales data with orders, customers, products).  
  > Dataset: [Superstore Sales dataset](https://www.kaggle.com/datasets/vivek468/superstore-dataset-final) *(replace if different)*  

Columns include:  
`Row ID, Order ID, Order Date, Ship Date, Ship Mode, Customer ID, Customer Name, Segment, Country, City, State, Postal Code, Region, Product ID, Category, Sub-Category, Product Name, Sales`

---

## ğŸš€ Project Features
- âœ… Ingest CSV sales data into PostgreSQL  
- âœ… Staging, dimension, and fact models with dbt  
- âœ… Data quality tests (not null, unique, relationships)  
- âœ… Airflow DAG (`dbt_dag`) orchestrates dbt runs + tests  
- âœ… Dockerized setup for easy deployment  

---

## ğŸ—‚ï¸ dbt Models
- **Staging**: `stg_orders`  
- **Dimensions**: `dim_customers`, `dim_products`  
- **Fact**: `fact_sales`  

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repo
```bash
git clone https://github.com/yourusername/Sales_Data_Pipeline.git
cd Sales_Data_Pipeline
